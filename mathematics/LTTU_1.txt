import numpy as np

def gradient_method_quadratic(A, b, x0, epsilon):
    """
    Gradient method for minimizing the quadratic function f(x) = x.T @ A @ x + 2 * b.T @ x.
    
    Parameters:
    A (numpy.ndarray): The positive definite matrix associated with the objective function.
    b (numpy.ndarray): A column vector associated with the linear part of the objective function.
    x0 (numpy.ndarray): Starting point of the method.
    epsilon (float): Tolerance parameter for the stopping rule.
    
    Returns:
    x (numpy.ndarray): An optimal solution (up to a tolerance) of min(x.T @ A @ x + 2 * b.T @ x).
    fun_val (float): The optimal function value up to a tolerance.
    """
    
    x = x0
    iter_count = 0
    grad = 2 * (A @ x + b)
    
    while np.linalg.norm(grad) > epsilon:
        iter_count += 1
        t = np.linalg.norm(grad)**2 / (2 * grad.T @ A @ grad)
        x = x - t * grad
        grad = 2 * (A @ x + b)
        fun_val = x.T @ A @ x + 2 * b.T @ x
        
        # Print the iteration number, gradient norm, and function value
        print(f'iter_number = {iter_count:3d} norm_grad = {np.linalg.norm(grad):.6f} fun_val = {fun_val:.6f}')
    
    return x, fun_val
      
# Call the gradient_method_quadratic function
x, fun_val = gradient_method_quadratic(A, b, x0, epsilon)

# Print the results
print("Optimal solution:", x)
print("Optimal function value:", fun_val)
  
# input 1
A = np.array([[1, 0], [0, 2]])
b = np.array([0, 0])
x0 = np.array([2.0, 1.0])  # Initial point
epsilon = 1e-5             # Tolerance parameter for stopping

# output 1
iter_number =   1 norm_grad = 1.885618 fun_val = 0.666667
iter_number =   2 norm_grad = 0.628539 fun_val = 0.074074
iter_number =   3 norm_grad = 0.209513 fun_val = 0.008230
iter_number =   4 norm_grad = 0.069838 fun_val = 0.000914
iter_number =   5 norm_grad = 0.023279 fun_val = 0.000102
iter_number =   6 norm_grad = 0.007760 fun_val = 0.000011
iter_number =   7 norm_grad = 0.002587 fun_val = 0.000001
iter_number =   8 norm_grad = 0.000862 fun_val = 0.000000
iter_number =   9 norm_grad = 0.000287 fun_val = 0.000000
iter_number =  10 norm_grad = 0.000096 fun_val = 0.000000
iter_number =  11 norm_grad = 0.000032 fun_val = 0.000000
iter_number =  12 norm_grad = 0.000011 fun_val = 0.000000
iter_number =  13 norm_grad = 0.000004 fun_val = 0.000000
Optimal solution: [ 1.25445095e-06 -6.27225474e-07]
Optimal function value: 2.360470774314766e-12

# input 2 
A = np.array([[1, 0], [0, 2]])
b = np.array([0, 0])
x0 = np.array([2.0, 1.0])  # Initial point
epsilon = 1e-6             # Tolerance parameter for stopping

# output 2
iter_number =   1 norm_grad = 1.885618 fun_val = 0.666667
iter_number =   2 norm_grad = 0.628539 fun_val = 0.074074
iter_number =   3 norm_grad = 0.209513 fun_val = 0.008230
iter_number =   4 norm_grad = 0.069838 fun_val = 0.000914
iter_number =   5 norm_grad = 0.023279 fun_val = 0.000102
iter_number =   6 norm_grad = 0.007760 fun_val = 0.000011
iter_number =   7 norm_grad = 0.002587 fun_val = 0.000001
iter_number =   8 norm_grad = 0.000862 fun_val = 0.000000
iter_number =   9 norm_grad = 0.000287 fun_val = 0.000000
iter_number =  10 norm_grad = 0.000096 fun_val = 0.000000
iter_number =  11 norm_grad = 0.000032 fun_val = 0.000000
iter_number =  12 norm_grad = 0.000011 fun_val = 0.000000
iter_number =  13 norm_grad = 0.000004 fun_val = 0.000000
iter_number =  14 norm_grad = 0.000001 fun_val = 0.000000
iter_number =  15 norm_grad = 0.000000 fun_val = 0.000000
Optimal solution: [ 1.39383439e-07 -6.96917194e-08]
Optimal function value: 2.914161449771316e-14
